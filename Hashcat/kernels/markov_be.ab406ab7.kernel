//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: UNKNOWN
// Driver 
// Based on LLVM 3.4svn
//

.version 5.0
.target sm_52, texmode_independent
.address_size 64

	// .globl	l_markov

.entry l_markov(
	.param .u64 .ptr .global .align 4 l_markov_param_0,
	.param .u64 .ptr .global .align 4 l_markov_param_1,
	.param .u64 .ptr .global .align 4 l_markov_param_2,
	.param .u64 l_markov_param_3,
	.param .u32 l_markov_param_4,
	.param .u32 l_markov_param_5,
	.param .u32 l_markov_param_6,
	.param .u32 l_markov_param_7,
	.param .u32 l_markov_param_8,
	.param .u32 l_markov_param_9
)
{
	.local .align 16 .b8 	__local_depot0[64];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<17>;
	.reg .b32 	%r<211>;
	.reg .b64 	%rd<184>;


	mov.u64 	%rd183, __local_depot0;
	cvta.local.u64 	%SP, %rd183;
	ld.param.u64 	%rd82, [l_markov_param_0];
	ld.param.u64 	%rd83, [l_markov_param_1];
	ld.param.u64 	%rd84, [l_markov_param_2];
	ld.param.u64 	%rd85, [l_markov_param_3];
	ld.param.u32 	%r31, [l_markov_param_4];
	ld.param.u32 	%r32, [l_markov_param_5];
	ld.param.u32 	%r33, [l_markov_param_6];
	ld.param.u32 	%r34, [l_markov_param_7];
	ld.param.u32 	%r35, [l_markov_param_8];
	ld.param.u32 	%r36, [l_markov_param_9];
	add.u64 	%rd86, %SP, 0;
	cvta.to.local.u64 	%rd1, %rd86;
	mov.u32 	%r37, %ctaid.x;
	mov.u32 	%r38, %ntid.x;
	mov.b32	%r39, %envreg3;
	mad.lo.s32 	%r40, %r37, %r38, %r39;
	mov.u32 	%r41, %tid.x;
	add.s32 	%r1, %r40, %r41;
	setp.ge.u32	%p1, %r1, %r36;
	@%p1 bra 	BB0_37;

	cvt.u64.u32	%rd2, %r1;
	add.s64 	%rd157, %rd2, %rd85;
	mov.u64 	%rd179, 0;
	st.local.v2.u64 	[%rd1], {%rd179, %rd179};
	st.local.v2.u64 	[%rd1+16], {%rd179, %rd179};
	st.local.v2.u64 	[%rd1+32], {%rd179, %rd179};
	st.local.v2.u64 	[%rd1+48], {%rd179, %rd179};
	cvt.u64.u32	%rd5, %r32;
	not.b32 	%r42, %r32;
	and.b32  	%r43, %r42, 3;
	shl.b32 	%r204, %r43, 3;
	and.b32  	%r44, %r32, -4;
	cvt.u64.u32	%rd88, %r44;
	add.s64 	%rd177, %rd1, %rd88;
	setp.eq.s32	%p2, %r31, 0;
	@%p2 bra 	BB0_31;

	mul.lo.s64 	%rd90, %rd5, 1028;
	add.s64 	%rd147, %rd83, %rd90;
	and.b32  	%r49, %r31, 3;
	mov.u32 	%r188, 1;
	mov.u32 	%r187, 0;
	mov.u32 	%r206, %r187;
	mov.u32 	%r196, %r187;
	setp.eq.s32	%p3, %r49, 0;
	mov.u32 	%r193, %r32;
	mov.u64 	%rd180, %rd177;
	mov.u32 	%r207, %r204;
	@%p3 bra 	BB0_17;

	setp.eq.s32	%p4, %r49, 1;
	mov.u32 	%r194, %r32;
	mov.u64 	%rd159, %rd157;
	mov.u64 	%rd181, %rd177;
	mov.u32 	%r208, %r204;
	@%p4 bra 	BB0_13;

	setp.eq.s32	%p5, %r49, 2;
	mov.u32 	%r195, %r32;
	mov.u64 	%rd161, %rd157;
	mov.u64 	%rd182, %rd177;
	mov.u32 	%r209, %r204;
	@%p5 bra 	BB0_9;

	add.s64 	%rd92, %rd83, %rd90;
	ld.global.u32 	%rd8, [%rd92+1024];
	and.b64  	%rd93, %rd157, -4294967296;
	setp.eq.s64	%p6, %rd93, 0;
	@%p6 bra 	BB0_7;

	div.u64 	%rd162, %rd157, %rd8;
	rem.u64 	%rd144, %rd157, %rd8;
	bra.uni 	BB0_8;

BB0_7:
	cvt.u32.u64	%r50, %rd8;
	cvt.u32.u64	%r51, %rd157;
	div.u32 	%r52, %r51, %r50;
	rem.u32 	%r53, %r51, %r50;
	cvt.u64.u32	%rd162, %r52;
	cvt.u64.u32	%rd144, %r53;

BB0_8:
	mov.u64 	%rd161, %rd162;
	shl.b64 	%rd96, %rd144, 2;
	add.s64 	%rd97, %rd92, %rd96;
	ld.global.u32 	%r55, [%rd97];
	shl.b32 	%r56, %r55, %r204;
	ld.local.u32 	%r57, [%rd177];
	or.b32  	%r58, %r57, %r56;
	st.local.u32 	[%rd177], %r58;
	shl.b32 	%r59, %r32, 8;
	add.s32 	%r60, %r55, %r59;
	mul.wide.u32 	%rd98, %r60, 1028;
	add.s64 	%rd147, %rd84, %rd98;
	add.s32 	%r195, %r32, 1;
	not.b32 	%r61, %r195;
	and.b32  	%r62, %r61, 3;
	shl.b32 	%r209, %r62, 3;
	and.b32  	%r63, %r195, -4;
	cvt.u64.u32	%rd99, %r63;
	add.s64 	%rd182, %rd1, %rd99;
	mov.u32 	%r188, 2;

BB0_9:
	mov.u32 	%r187, %r188;
	ld.global.u32 	%rd20, [%rd147+1024];
	and.b64  	%rd100, %rd161, -4294967296;
	setp.eq.s64	%p7, %rd100, 0;
	@%p7 bra 	BB0_11;

	div.u64 	%rd160, %rd161, %rd20;
	rem.u64 	%rd145, %rd161, %rd20;
	bra.uni 	BB0_12;

BB0_11:
	cvt.u32.u64	%r64, %rd20;
	cvt.u32.u64	%r65, %rd161;
	div.u32 	%r66, %r65, %r64;
	rem.u32 	%r67, %r65, %r64;
	cvt.u64.u32	%rd160, %r66;
	cvt.u64.u32	%rd145, %r67;

BB0_12:
	mov.u64 	%rd159, %rd160;
	shl.b64 	%rd101, %rd145, 2;
	add.s64 	%rd102, %rd147, %rd101;
	ld.global.u32 	%r68, [%rd102];
	shl.b32 	%r69, %r68, %r209;
	ld.local.u32 	%r70, [%rd182];
	or.b32  	%r71, %r70, %r69;
	st.local.u32 	[%rd182], %r71;
	shl.b32 	%r72, %r195, 8;
	add.s32 	%r73, %r68, %r72;
	mul.wide.u32 	%rd103, %r73, 1028;
	add.s64 	%rd147, %rd84, %rd103;
	add.s32 	%r194, %r195, 1;
	not.b32 	%r74, %r194;
	and.b32  	%r75, %r74, 3;
	shl.b32 	%r208, %r75, 3;
	and.b32  	%r76, %r194, -4;
	cvt.u64.u32	%rd104, %r76;
	add.s64 	%rd181, %rd1, %rd104;

BB0_13:
	ld.global.u32 	%rd32, [%rd147+1024];
	and.b64  	%rd105, %rd159, -4294967296;
	setp.eq.s64	%p8, %rd105, 0;
	@%p8 bra 	BB0_15;

	div.u64 	%rd158, %rd159, %rd32;
	rem.u64 	%rd146, %rd159, %rd32;
	bra.uni 	BB0_16;

BB0_15:
	cvt.u32.u64	%r77, %rd32;
	cvt.u32.u64	%r78, %rd159;
	div.u32 	%r79, %r78, %r77;
	rem.u32 	%r80, %r78, %r77;
	cvt.u64.u32	%rd158, %r79;
	cvt.u64.u32	%rd146, %r80;

BB0_16:
	mov.u64 	%rd157, %rd158;
	shl.b64 	%rd106, %rd146, 2;
	add.s64 	%rd107, %rd147, %rd106;
	ld.global.u32 	%r81, [%rd107];
	shl.b32 	%r82, %r81, %r208;
	ld.local.u32 	%r83, [%rd181];
	or.b32  	%r84, %r83, %r82;
	st.local.u32 	[%rd181], %r84;
	shl.b32 	%r85, %r194, 8;
	add.s32 	%r86, %r81, %r85;
	mul.wide.u32 	%rd108, %r86, 1028;
	add.s64 	%rd147, %rd84, %rd108;
	add.s32 	%r196, %r187, 1;
	add.s32 	%r14, %r194, 1;
	not.b32 	%r87, %r14;
	and.b32  	%r88, %r87, 3;
	shl.b32 	%r207, %r88, 3;
	and.b32  	%r89, %r14, -4;
	cvt.u64.u32	%rd109, %r89;
	add.s64 	%rd180, %rd1, %rd109;
	mov.u32 	%r193, %r14;
	mov.u64 	%rd179, %rd180;
	mov.u32 	%r206, %r207;

BB0_17:
	mov.u32 	%r204, %r206;
	mov.u32 	%r205, %r207;
	mov.u64 	%rd177, %rd179;
	mov.u64 	%rd178, %rd180;
	mov.u64 	%rd155, %rd157;
	mov.u32 	%r191, %r193;
	mov.u32 	%r192, %r191;
	setp.lt.u32	%p9, %r31, 4;
	@%p9 bra 	BB0_31;

BB0_18:
	ld.global.u32 	%rd48, [%rd147+1024];
	and.b64  	%rd110, %rd155, -4294967296;
	setp.eq.s64	%p10, %rd110, 0;
	@%p10 bra 	BB0_20;
	bra.uni 	BB0_19;

BB0_20:
	cvt.u32.u64	%r90, %rd48;
	cvt.u32.u64	%r91, %rd155;
	div.u32 	%r92, %r91, %r90;
	rem.u32 	%r93, %r91, %r90;
	cvt.u64.u32	%rd163, %r92;
	cvt.u64.u32	%rd164, %r93;
	bra.uni 	BB0_21;

BB0_19:
	div.u64 	%rd163, %rd155, %rd48;
	rem.u64 	%rd164, %rd155, %rd48;

BB0_21:
	shl.b64 	%rd111, %rd164, 2;
	add.s64 	%rd112, %rd147, %rd111;
	ld.global.u32 	%r94, [%rd112];
	shl.b32 	%r95, %r94, %r205;
	ld.local.u32 	%r96, [%rd178];
	or.b32  	%r97, %r96, %r95;
	st.local.u32 	[%rd178], %r97;
	shl.b32 	%r98, %r192, 8;
	add.s32 	%r99, %r94, %r98;
	cvt.u64.u32	%rd55, %r99;
	mul.wide.u32 	%rd113, %r99, 1028;
	add.s64 	%rd114, %rd84, %rd113;
	ld.global.u32 	%rd56, [%rd114+1024];
	and.b64  	%rd115, %rd163, -4294967296;
	setp.eq.s64	%p11, %rd115, 0;
	@%p11 bra 	BB0_23;
	bra.uni 	BB0_22;

BB0_23:
	cvt.u32.u64	%r100, %rd56;
	cvt.u32.u64	%r101, %rd163;
	div.u32 	%r102, %r101, %r100;
	rem.u32 	%r103, %r101, %r100;
	cvt.u64.u32	%rd165, %r102;
	cvt.u64.u32	%rd166, %r103;
	bra.uni 	BB0_24;

BB0_22:
	div.u64 	%rd165, %rd163, %rd56;
	rem.u64 	%rd166, %rd163, %rd56;

BB0_24:
	add.s32 	%r104, %r192, 1;
	not.b32 	%r105, %r104;
	and.b32  	%r106, %r105, 3;
	shl.b32 	%r107, %r106, 3;
	and.b32  	%r108, %r104, -4;
	cvt.u64.u32	%rd116, %r108;
	add.s64 	%rd117, %rd1, %rd116;
	mul.lo.s64 	%rd118, %rd55, 1028;
	add.s64 	%rd119, %rd84, %rd118;
	shl.b64 	%rd120, %rd166, 2;
	add.s64 	%rd121, %rd119, %rd120;
	ld.global.u32 	%r109, [%rd121];
	shl.b32 	%r110, %r109, %r107;
	ld.local.u32 	%r111, [%rd117];
	or.b32  	%r112, %r111, %r110;
	st.local.u32 	[%rd117], %r112;
	shl.b32 	%r113, %r104, 8;
	add.s32 	%r114, %r109, %r113;
	cvt.u64.u32	%rd63, %r114;
	mul.wide.u32 	%rd122, %r114, 1028;
	add.s64 	%rd123, %rd84, %rd122;
	ld.global.u32 	%rd64, [%rd123+1024];
	and.b64  	%rd124, %rd165, -4294967296;
	setp.eq.s64	%p12, %rd124, 0;
	@%p12 bra 	BB0_26;
	bra.uni 	BB0_25;

BB0_26:
	cvt.u32.u64	%r115, %rd64;
	cvt.u32.u64	%r116, %rd165;
	div.u32 	%r117, %r116, %r115;
	rem.u32 	%r118, %r116, %r115;
	cvt.u64.u32	%rd167, %r117;
	cvt.u64.u32	%rd168, %r118;
	bra.uni 	BB0_27;

BB0_25:
	div.u64 	%rd167, %rd165, %rd64;
	rem.u64 	%rd168, %rd165, %rd64;

BB0_27:
	add.s32 	%r119, %r192, 2;
	not.b32 	%r120, %r119;
	and.b32  	%r121, %r120, 3;
	shl.b32 	%r122, %r121, 3;
	and.b32  	%r123, %r119, -4;
	cvt.u64.u32	%rd125, %r123;
	add.s64 	%rd126, %rd1, %rd125;
	mul.lo.s64 	%rd127, %rd63, 1028;
	add.s64 	%rd128, %rd84, %rd127;
	shl.b64 	%rd129, %rd168, 2;
	add.s64 	%rd130, %rd128, %rd129;
	ld.global.u32 	%r124, [%rd130];
	shl.b32 	%r125, %r124, %r122;
	ld.local.u32 	%r126, [%rd126];
	or.b32  	%r127, %r126, %r125;
	st.local.u32 	[%rd126], %r127;
	shl.b32 	%r128, %r119, 8;
	add.s32 	%r129, %r124, %r128;
	cvt.u64.u32	%rd71, %r129;
	mul.wide.u32 	%rd131, %r129, 1028;
	add.s64 	%rd132, %rd84, %rd131;
	ld.global.u32 	%rd72, [%rd132+1024];
	and.b64  	%rd133, %rd167, -4294967296;
	setp.eq.s64	%p13, %rd133, 0;
	@%p13 bra 	BB0_29;
	bra.uni 	BB0_28;

BB0_29:
	cvt.u32.u64	%r130, %rd72;
	cvt.u32.u64	%r131, %rd167;
	div.u32 	%r132, %r131, %r130;
	rem.u32 	%r133, %r131, %r130;
	cvt.u64.u32	%rd156, %r132;
	cvt.u64.u32	%rd169, %r133;
	bra.uni 	BB0_30;

BB0_28:
	div.u64 	%rd156, %rd167, %rd72;
	rem.u64 	%rd169, %rd167, %rd72;

BB0_30:
	mov.u64 	%rd155, %rd156;
	add.s32 	%r134, %r192, 3;
	shl.b32 	%r135, %r134, 3;
	not.b32 	%r136, %r135;
	and.b32  	%r137, %r136, 24;
	and.b32  	%r138, %r134, -4;
	cvt.u64.u32	%rd134, %r138;
	add.s64 	%rd135, %rd1, %rd134;
	mul.lo.s64 	%rd136, %rd71, 1028;
	add.s64 	%rd137, %rd84, %rd136;
	shl.b64 	%rd138, %rd169, 2;
	add.s64 	%rd139, %rd137, %rd138;
	ld.global.u32 	%r139, [%rd139];
	shl.b32 	%r140, %r139, %r137;
	ld.local.u32 	%r141, [%rd135];
	or.b32  	%r142, %r141, %r140;
	st.local.u32 	[%rd135], %r142;
	shl.b32 	%r143, %r134, 8;
	add.s32 	%r144, %r139, %r143;
	mul.wide.u32 	%rd140, %r144, 1028;
	add.s64 	%rd147, %rd84, %rd140;
	add.s32 	%r196, %r196, 4;
	setp.lt.u32	%p14, %r196, %r31;
	add.s32 	%r192, %r192, 4;
	shl.b32 	%r145, %r192, 3;
	not.b32 	%r146, %r145;
	and.b32  	%r205, %r146, 24;
	and.b32  	%r147, %r192, -4;
	cvt.u64.u32	%rd141, %r147;
	add.s64 	%rd178, %rd1, %rd141;
	mov.u64 	%rd177, %rd178;
	mov.u32 	%r204, %r205;
	@%p14 bra 	BB0_18;

BB0_31:
	mov.u32 	%r148, 255;
	shl.b32 	%r149, %r148, %r204;
	and.b32  	%r150, %r149, %r33;
	ld.local.u32 	%r151, [%rd177];
	or.b32  	%r152, %r151, %r150;
	st.local.u32 	[%rd177], %r152;
	setp.eq.s32	%p15, %r34, 0;
	@%p15 bra 	BB0_33;

	add.s32 	%r153, %r32, %r31;
	shl.b32 	%r154, %r153, 3;
	st.local.u32 	[%rd1+56], %r154;

BB0_33:
	add.s32 	%r27, %r32, %r31;
	setp.eq.s32	%p16, %r35, 0;
	@%p16 bra 	BB0_35;

	shl.b32 	%r210, %r27, 3;
	st.local.u32 	[%rd1+60], %r210;
	bra.uni 	BB0_36;

BB0_35:
	ld.local.u32 	%r210, [%rd1+60];

BB0_36:
	mul.lo.s64 	%rd142, %rd2, 80;
	add.s64 	%rd143, %rd82, %rd142;
	ld.local.v4.u32 	{%r155, %r156, %r157, %r158}, [%rd1];
	ld.local.v4.u32 	{%r160, %r161, %r162, %r163}, [%rd1+16];
	ld.local.v4.u32 	{%r164, %r165, %r166, %r167}, [%rd1+32];
	ld.local.v4.u32 	{%r168, %r169, %r170, %r171}, [%rd1+48];
	st.global.u32 	[%rd143], %r155;
	st.global.u32 	[%rd143+4], %r156;
	st.global.u32 	[%rd143+8], %r157;
	st.global.u32 	[%rd143+12], %r158;
	st.global.u32 	[%rd143+16], %r160;
	st.global.u32 	[%rd143+20], %r161;
	st.global.u32 	[%rd143+24], %r162;
	st.global.u32 	[%rd143+28], %r163;
	st.global.u32 	[%rd143+32], %r164;
	st.global.u32 	[%rd143+36], %r165;
	st.global.u32 	[%rd143+40], %r166;
	st.global.u32 	[%rd143+44], %r167;
	st.global.u32 	[%rd143+48], %r168;
	st.global.u32 	[%rd143+52], %r169;
	st.global.u32 	[%rd143+56], %r170;
	st.global.u32 	[%rd143+60], %r210;
	st.global.u32 	[%rd143+64], %r27;

BB0_37:
	ret;
}

	// .globl	r_markov
.entry r_markov(
	.param .u64 .ptr .global .align 4 r_markov_param_0,
	.param .u64 .ptr .global .align 4 r_markov_param_1,
	.param .u64 .ptr .global .align 4 r_markov_param_2,
	.param .u64 r_markov_param_3,
	.param .u32 r_markov_param_4,
	.param .u32 r_markov_param_5,
	.param .u32 r_markov_param_6,
	.param .u32 r_markov_param_7,
	.param .u32 r_markov_param_8
)
{
	.local .align 16 .b8 	__local_depot1[64];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<15>;
	.reg .b32 	%r<135>;
	.reg .b64 	%rd<151>;


	mov.u64 	%rd150, __local_depot1;
	cvta.local.u64 	%SP, %rd150;
	ld.param.u64 	%rd69, [r_markov_param_0];
	ld.param.u64 	%rd127, [r_markov_param_1];
	ld.param.u64 	%rd71, [r_markov_param_2];
	ld.param.u64 	%rd72, [r_markov_param_3];
	ld.param.u32 	%r20, [r_markov_param_4];
	ld.param.u32 	%r21, [r_markov_param_8];
	add.u64 	%rd73, %SP, 0;
	cvta.to.local.u64 	%rd1, %rd73;
	mov.u32 	%r22, %ctaid.x;
	mov.u32 	%r23, %ntid.x;
	mov.b32	%r24, %envreg3;
	mad.lo.s32 	%r25, %r22, %r23, %r24;
	mov.u32 	%r26, %tid.x;
	add.s32 	%r1, %r25, %r26;
	setp.ge.u32	%p1, %r1, %r21;
	@%p1 bra 	BB1_33;

	cvt.u64.u32	%rd2, %r1;
	add.s64 	%rd137, %rd2, %rd72;
	mov.u64 	%rd74, 0;
	st.local.v2.u64 	[%rd1], {%rd74, %rd74};
	st.local.v2.u64 	[%rd1+16], {%rd74, %rd74};
	st.local.v2.u64 	[%rd1+32], {%rd74, %rd74};
	st.local.v2.u64 	[%rd1+48], {%rd74, %rd74};
	setp.eq.s32	%p2, %r20, 0;
	@%p2 bra 	BB1_32;

	and.b32  	%r34, %r20, 3;
	mov.u32 	%r131, 1;
	mov.u32 	%r127, 0;
	mov.u32 	%r126, %r127;
	mov.u32 	%r130, %r127;
	mov.u32 	%r128, %r127;
	mov.u32 	%r134, %r127;
	mov.u32 	%r133, %r127;
	setp.eq.s32	%p3, %r34, 0;
	@%p3 bra 	BB1_17;

	setp.eq.s32	%p4, %r34, 1;
	mov.u64 	%rd139, %rd137;
	@%p4 bra 	BB1_13;

	setp.eq.s32	%p5, %r34, 2;
	mov.u64 	%rd141, %rd137;
	@%p5 bra 	BB1_9;

	ld.global.u32 	%rd5, [%rd127+1024];
	and.b64  	%rd75, %rd137, -4294967296;
	setp.eq.s64	%p6, %rd75, 0;
	@%p6 bra 	BB1_7;

	div.u64 	%rd142, %rd137, %rd5;
	rem.u64 	%rd124, %rd137, %rd5;
	bra.uni 	BB1_8;

BB1_7:
	cvt.u32.u64	%r35, %rd5;
	cvt.u32.u64	%r36, %rd137;
	div.u32 	%r37, %r36, %r35;
	rem.u32 	%r38, %r36, %r35;
	cvt.u64.u32	%rd142, %r37;
	cvt.u64.u32	%rd124, %r38;

BB1_8:
	mov.u64 	%rd141, %rd142;
	shl.b64 	%rd76, %rd124, 2;
	add.s64 	%rd77, %rd127, %rd76;
	ld.global.u32 	%r41, [%rd77];
	shl.b32 	%r126, %r41, 24;
	st.local.u32 	[%rd1], %r126;
	mul.wide.u32 	%rd78, %r41, 1028;
	add.s64 	%rd127, %rd71, %rd78;
	mov.u32 	%r131, 2;
	mov.u32 	%r127, 1;

BB1_9:
	mov.u32 	%r130, %r131;
	ld.global.u32 	%rd15, [%rd127+1024];
	and.b64  	%rd79, %rd141, -4294967296;
	setp.eq.s64	%p7, %rd79, 0;
	@%p7 bra 	BB1_11;

	div.u64 	%rd140, %rd141, %rd15;
	rem.u64 	%rd125, %rd141, %rd15;
	bra.uni 	BB1_12;

BB1_11:
	cvt.u32.u64	%r42, %rd15;
	cvt.u32.u64	%r43, %rd141;
	div.u32 	%r44, %r43, %r42;
	rem.u32 	%r45, %r43, %r42;
	cvt.u64.u32	%rd140, %r44;
	cvt.u64.u32	%rd125, %r45;

BB1_12:
	mov.u64 	%rd139, %rd140;
	xor.b32  	%r46, %r127, 3;
	shl.b32 	%r47, %r46, 3;
	shl.b64 	%rd80, %rd125, 2;
	add.s64 	%rd81, %rd127, %rd80;
	ld.global.u32 	%r48, [%rd81];
	shl.b32 	%r49, %r48, %r47;
	or.b32  	%r50, %r126, %r49;
	st.local.u32 	[%rd1], %r50;
	shl.b32 	%r51, %r127, 8;
	add.s32 	%r52, %r48, %r51;
	mul.wide.u32 	%rd82, %r52, 1028;
	add.s64 	%rd127, %rd71, %rd82;
	add.s32 	%r128, %r127, 1;

BB1_13:
	ld.global.u32 	%rd25, [%rd127+1024];
	and.b64  	%rd83, %rd139, -4294967296;
	setp.eq.s64	%p8, %rd83, 0;
	@%p8 bra 	BB1_15;

	div.u64 	%rd138, %rd139, %rd25;
	rem.u64 	%rd126, %rd139, %rd25;
	bra.uni 	BB1_16;

BB1_15:
	cvt.u32.u64	%r53, %rd25;
	cvt.u32.u64	%r54, %rd139;
	div.u32 	%r55, %r54, %r53;
	rem.u32 	%r56, %r54, %r53;
	cvt.u64.u32	%rd138, %r55;
	cvt.u64.u32	%rd126, %r56;

BB1_16:
	mov.u64 	%rd137, %rd138;
	not.b32 	%r57, %r128;
	and.b32  	%r58, %r57, 3;
	shl.b32 	%r59, %r58, 3;
	shl.b64 	%rd84, %rd126, 2;
	add.s64 	%rd85, %rd127, %rd84;
	ld.global.u32 	%r60, [%rd85];
	shl.b32 	%r61, %r60, %r59;
	and.b32  	%r62, %r128, -4;
	cvt.u64.u32	%rd86, %r62;
	add.s64 	%rd87, %rd1, %rd86;
	ld.local.u32 	%r63, [%rd87];
	or.b32  	%r64, %r63, %r61;
	st.local.u32 	[%rd87], %r64;
	shl.b32 	%r65, %r128, 8;
	add.s32 	%r66, %r60, %r65;
	mul.wide.u32 	%rd88, %r66, 1028;
	add.s64 	%rd127, %rd71, %rd88;
	add.s32 	%r134, %r130, 1;
	add.s32 	%r133, %r128, 1;

BB1_17:
	mov.u64 	%rd135, %rd137;
	setp.lt.u32	%p9, %r20, 4;
	@%p9 bra 	BB1_32;

	shl.b32 	%r132, %r133, 8;

BB1_19:
	ld.global.u32 	%rd37, [%rd127+1024];
	and.b64  	%rd89, %rd135, -4294967296;
	setp.eq.s64	%p10, %rd89, 0;
	@%p10 bra 	BB1_21;
	bra.uni 	BB1_20;

BB1_21:
	cvt.u32.u64	%r67, %rd37;
	cvt.u32.u64	%r68, %rd135;
	div.u32 	%r69, %r68, %r67;
	rem.u32 	%r70, %r68, %r67;
	cvt.u64.u32	%rd143, %r69;
	cvt.u64.u32	%rd144, %r70;
	bra.uni 	BB1_22;

BB1_20:
	div.u64 	%rd143, %rd135, %rd37;
	rem.u64 	%rd144, %rd135, %rd37;

BB1_22:
	not.b32 	%r71, %r133;
	and.b32  	%r72, %r71, 3;
	shl.b32 	%r73, %r72, 3;
	shl.b64 	%rd90, %rd144, 2;
	add.s64 	%rd91, %rd127, %rd90;
	ld.global.u32 	%r74, [%rd91];
	shl.b32 	%r75, %r74, %r73;
	and.b32  	%r76, %r133, -4;
	cvt.u64.u32	%rd92, %r76;
	add.s64 	%rd93, %rd1, %rd92;
	ld.local.u32 	%r77, [%rd93];
	or.b32  	%r78, %r77, %r75;
	st.local.u32 	[%rd93], %r78;
	add.s32 	%r79, %r132, %r74;
	cvt.u64.u32	%rd44, %r79;
	mul.wide.u32 	%rd94, %r79, 1028;
	add.s64 	%rd95, %rd71, %rd94;
	ld.global.u32 	%rd45, [%rd95+1024];
	and.b64  	%rd96, %rd143, -4294967296;
	setp.eq.s64	%p11, %rd96, 0;
	@%p11 bra 	BB1_24;
	bra.uni 	BB1_23;

BB1_24:
	cvt.u32.u64	%r80, %rd45;
	cvt.u32.u64	%r81, %rd143;
	div.u32 	%r82, %r81, %r80;
	rem.u32 	%r83, %r81, %r80;
	cvt.u64.u32	%rd145, %r82;
	cvt.u64.u32	%rd146, %r83;
	bra.uni 	BB1_25;

BB1_23:
	div.u64 	%rd145, %rd143, %rd45;
	rem.u64 	%rd146, %rd143, %rd45;

BB1_25:
	add.s32 	%r84, %r133, 1;
	not.b32 	%r85, %r84;
	and.b32  	%r86, %r85, 3;
	shl.b32 	%r87, %r86, 3;
	mul.lo.s64 	%rd97, %rd44, 1028;
	add.s64 	%rd98, %rd71, %rd97;
	shl.b64 	%rd99, %rd146, 2;
	add.s64 	%rd100, %rd98, %rd99;
	ld.global.u32 	%r88, [%rd100];
	shl.b32 	%r89, %r88, %r87;
	and.b32  	%r90, %r84, -4;
	cvt.u64.u32	%rd101, %r90;
	add.s64 	%rd102, %rd1, %rd101;
	ld.local.u32 	%r91, [%rd102];
	or.b32  	%r92, %r91, %r89;
	st.local.u32 	[%rd102], %r92;
	add.s32 	%r93, %r132, %r88;
	add.s32 	%r94, %r93, 256;
	cvt.u64.u32	%rd52, %r94;
	mul.wide.u32 	%rd103, %r94, 1028;
	add.s64 	%rd104, %rd71, %rd103;
	ld.global.u32 	%rd53, [%rd104+1024];
	and.b64  	%rd105, %rd145, -4294967296;
	setp.eq.s64	%p12, %rd105, 0;
	@%p12 bra 	BB1_27;
	bra.uni 	BB1_26;

BB1_27:
	cvt.u32.u64	%r95, %rd53;
	cvt.u32.u64	%r96, %rd145;
	div.u32 	%r97, %r96, %r95;
	rem.u32 	%r98, %r96, %r95;
	cvt.u64.u32	%rd147, %r97;
	cvt.u64.u32	%rd148, %r98;
	bra.uni 	BB1_28;

BB1_26:
	div.u64 	%rd147, %rd145, %rd53;
	rem.u64 	%rd148, %rd145, %rd53;

BB1_28:
	add.s32 	%r99, %r133, 2;
	not.b32 	%r100, %r99;
	and.b32  	%r101, %r100, 3;
	shl.b32 	%r102, %r101, 3;
	mul.lo.s64 	%rd106, %rd52, 1028;
	add.s64 	%rd107, %rd71, %rd106;
	shl.b64 	%rd108, %rd148, 2;
	add.s64 	%rd109, %rd107, %rd108;
	ld.global.u32 	%r103, [%rd109];
	shl.b32 	%r104, %r103, %r102;
	and.b32  	%r105, %r99, -4;
	cvt.u64.u32	%rd110, %r105;
	add.s64 	%rd111, %rd1, %rd110;
	ld.local.u32 	%r106, [%rd111];
	or.b32  	%r107, %r106, %r104;
	st.local.u32 	[%rd111], %r107;
	add.s32 	%r108, %r132, %r103;
	add.s32 	%r109, %r108, 512;
	cvt.u64.u32	%rd60, %r109;
	mul.wide.u32 	%rd112, %r109, 1028;
	add.s64 	%rd113, %rd71, %rd112;
	ld.global.u32 	%rd61, [%rd113+1024];
	and.b64  	%rd114, %rd147, -4294967296;
	setp.eq.s64	%p13, %rd114, 0;
	@%p13 bra 	BB1_30;
	bra.uni 	BB1_29;

BB1_30:
	cvt.u32.u64	%r110, %rd61;
	cvt.u32.u64	%r111, %rd147;
	div.u32 	%r112, %r111, %r110;
	rem.u32 	%r113, %r111, %r110;
	cvt.u64.u32	%rd136, %r112;
	cvt.u64.u32	%rd149, %r113;
	bra.uni 	BB1_31;

BB1_29:
	div.u64 	%rd136, %rd147, %rd61;
	rem.u64 	%rd149, %rd147, %rd61;

BB1_31:
	mov.u64 	%rd135, %rd136;
	add.s32 	%r114, %r133, 3;
	not.b32 	%r115, %r114;
	and.b32  	%r116, %r115, 3;
	shl.b32 	%r117, %r116, 3;
	mul.lo.s64 	%rd115, %rd60, 1028;
	add.s64 	%rd116, %rd71, %rd115;
	shl.b64 	%rd117, %rd149, 2;
	add.s64 	%rd118, %rd116, %rd117;
	ld.global.u32 	%r118, [%rd118];
	shl.b32 	%r119, %r118, %r117;
	and.b32  	%r120, %r114, -4;
	cvt.u64.u32	%rd119, %r120;
	add.s64 	%rd120, %rd1, %rd119;
	ld.local.u32 	%r121, [%rd120];
	or.b32  	%r122, %r121, %r119;
	st.local.u32 	[%rd120], %r122;
	add.s32 	%r123, %r132, %r118;
	add.s32 	%r124, %r123, 768;
	mul.wide.u32 	%rd121, %r124, 1028;
	add.s64 	%rd127, %rd71, %rd121;
	add.s32 	%r133, %r133, 4;
	add.s32 	%r132, %r132, 1024;
	add.s32 	%r134, %r134, 4;
	setp.lt.u32	%p14, %r134, %r20;
	@%p14 bra 	BB1_19;

BB1_32:
	ld.local.u32 	%r125, [%rd1];
	shl.b64 	%rd122, %rd2, 2;
	add.s64 	%rd123, %rd69, %rd122;
	st.global.u32 	[%rd123], %r125;

BB1_33:
	ret;
}

	// .globl	C_markov
.entry C_markov(
	.param .u64 .ptr .global .align 4 C_markov_param_0,
	.param .u64 .ptr .global .align 4 C_markov_param_1,
	.param .u64 .ptr .global .align 4 C_markov_param_2,
	.param .u64 C_markov_param_3,
	.param .u32 C_markov_param_4,
	.param .u32 C_markov_param_5,
	.param .u32 C_markov_param_6,
	.param .u32 C_markov_param_7,
	.param .u32 C_markov_param_8
)
{
	.local .align 16 .b8 	__local_depot2[64];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<17>;
	.reg .b32 	%r<184>;
	.reg .b64 	%rd<170>;


	mov.u64 	%rd169, __local_depot2;
	cvta.local.u64 	%SP, %rd169;
	ld.param.u64 	%rd78, [C_markov_param_0];
	ld.param.u64 	%rd136, [C_markov_param_1];
	ld.param.u64 	%rd80, [C_markov_param_2];
	ld.param.u64 	%rd81, [C_markov_param_3];
	ld.param.u32 	%r28, [C_markov_param_4];
	ld.param.u32 	%r29, [C_markov_param_5];
	ld.param.u32 	%r30, [C_markov_param_6];
	ld.param.u32 	%r31, [C_markov_param_7];
	ld.param.u32 	%r32, [C_markov_param_8];
	add.u64 	%rd82, %SP, 0;
	cvta.to.local.u64 	%rd1, %rd82;
	mov.u32 	%r33, %ctaid.x;
	mov.u32 	%r34, %ntid.x;
	mov.b32	%r35, %envreg3;
	mad.lo.s32 	%r36, %r33, %r34, %r35;
	mov.u32 	%r37, %tid.x;
	add.s32 	%r1, %r36, %r37;
	setp.ge.u32	%p1, %r1, %r32;
	@%p1 bra 	BB2_36;

	cvt.u64.u32	%rd2, %r1;
	add.s64 	%rd146, %rd2, %rd81;
	mov.u64 	%rd164, %rd1;
	mov.u64 	%rd166, 0;
	st.local.v2.u64 	[%rd1], {%rd166, %rd166};
	st.local.v2.u64 	[%rd1+16], {%rd166, %rd166};
	st.local.v2.u64 	[%rd1+32], {%rd166, %rd166};
	st.local.v2.u64 	[%rd1+48], {%rd166, %rd166};
	setp.eq.s32	%p2, %r28, 0;
	mov.u32 	%r180, 24;
	@%p2 bra 	BB2_31;

	mov.u64 	%rd167, %rd1;
	and.b32  	%r51, %r28, 3;
	mov.u32 	%r173, 1;
	mov.u32 	%r167, 0;
	mov.u32 	%r166, 24;
	mov.u32 	%r165, %r167;
	mov.u32 	%r172, %r167;
	mov.u32 	%r170, %r167;
	mov.u32 	%r169, %r166;
	mov.u32 	%r168, %r167;
	mov.u32 	%r182, %r167;
	mov.u32 	%r175, %r167;
	mov.u32 	%r174, %r167;
	mov.u32 	%r183, %r166;
	setp.eq.s32	%p3, %r51, 0;
	@%p3 bra 	BB2_17;

	setp.eq.s32	%p4, %r51, 1;
	mov.u64 	%rd148, %rd146;
	mov.u64 	%rd168, %rd167;
	@%p4 bra 	BB2_13;

	setp.eq.s32	%p5, %r51, 2;
	mov.u64 	%rd150, %rd146;
	@%p5 bra 	BB2_9;

	ld.global.u32 	%rd6, [%rd136+1024];
	and.b64  	%rd85, %rd146, -4294967296;
	setp.eq.s64	%p6, %rd85, 0;
	@%p6 bra 	BB2_7;

	div.u64 	%rd151, %rd146, %rd6;
	rem.u64 	%rd133, %rd146, %rd6;
	bra.uni 	BB2_8;

BB2_7:
	cvt.u32.u64	%r52, %rd6;
	cvt.u32.u64	%r53, %rd146;
	div.u32 	%r54, %r53, %r52;
	rem.u32 	%r55, %r53, %r52;
	cvt.u64.u32	%rd151, %r54;
	cvt.u64.u32	%rd133, %r55;

BB2_8:
	mov.u64 	%rd150, %rd151;
	shl.b64 	%rd86, %rd133, 2;
	add.s64 	%rd87, %rd136, %rd86;
	ld.global.u32 	%r59, [%rd87];
	shl.b32 	%r165, %r59, 24;
	st.local.u32 	[%rd1], %r165;
	mul.wide.u32 	%rd88, %r59, 1028;
	add.s64 	%rd136, %rd80, %rd88;
	mov.u32 	%r173, 2;
	mov.u32 	%r167, 1;
	mov.u32 	%r166, 16;

BB2_9:
	mov.u32 	%r172, %r173;
	ld.global.u32 	%rd16, [%rd136+1024];
	and.b64  	%rd89, %rd150, -4294967296;
	setp.eq.s64	%p7, %rd89, 0;
	@%p7 bra 	BB2_11;

	div.u64 	%rd149, %rd150, %rd16;
	rem.u64 	%rd134, %rd150, %rd16;
	bra.uni 	BB2_12;

BB2_11:
	cvt.u32.u64	%r60, %rd16;
	cvt.u32.u64	%r61, %rd150;
	div.u32 	%r62, %r61, %r60;
	rem.u32 	%r63, %r61, %r60;
	cvt.u64.u32	%rd149, %r62;
	cvt.u64.u32	%rd134, %r63;

BB2_12:
	mov.u64 	%rd148, %rd149;
	shl.b64 	%rd90, %rd134, 2;
	add.s64 	%rd91, %rd136, %rd90;
	ld.global.u32 	%r64, [%rd91];
	shl.b32 	%r65, %r64, %r166;
	or.b32  	%r66, %r165, %r65;
	st.local.u32 	[%rd1], %r66;
	shl.b32 	%r67, %r167, 8;
	add.s32 	%r68, %r64, %r67;
	mul.wide.u32 	%rd92, %r68, 1028;
	add.s64 	%rd136, %rd80, %rd92;
	add.s32 	%r170, %r167, 1;
	not.b32 	%r69, %r170;
	and.b32  	%r70, %r69, 3;
	shl.b32 	%r169, %r70, 3;
	and.b32  	%r71, %r170, -4;
	cvt.u64.u32	%rd93, %r71;
	add.s64 	%rd168, %rd1, %rd93;
	ld.local.u32 	%r168, [%rd168];

BB2_13:
	ld.global.u32 	%rd28, [%rd136+1024];
	and.b64  	%rd94, %rd148, -4294967296;
	setp.eq.s64	%p8, %rd94, 0;
	@%p8 bra 	BB2_15;

	div.u64 	%rd147, %rd148, %rd28;
	rem.u64 	%rd135, %rd148, %rd28;
	bra.uni 	BB2_16;

BB2_15:
	cvt.u32.u64	%r72, %rd28;
	cvt.u32.u64	%r73, %rd148;
	div.u32 	%r74, %r73, %r72;
	rem.u32 	%r75, %r73, %r72;
	cvt.u64.u32	%rd147, %r74;
	cvt.u64.u32	%rd135, %r75;

BB2_16:
	mov.u64 	%rd146, %rd147;
	shl.b64 	%rd95, %rd135, 2;
	add.s64 	%rd96, %rd136, %rd95;
	ld.global.u32 	%r76, [%rd96];
	shl.b32 	%r77, %r76, %r169;
	or.b32  	%r78, %r168, %r77;
	st.local.u32 	[%rd168], %r78;
	shl.b32 	%r79, %r170, 8;
	add.s32 	%r80, %r76, %r79;
	mul.wide.u32 	%rd97, %r80, 1028;
	add.s64 	%rd136, %rd80, %rd97;
	add.s32 	%r175, %r172, 1;
	add.s32 	%r174, %r170, 1;
	not.b32 	%r81, %r174;
	and.b32  	%r82, %r81, 3;
	shl.b32 	%r183, %r82, 3;
	and.b32  	%r83, %r174, -4;
	cvt.u64.u32	%rd98, %r83;
	add.s64 	%rd167, %rd1, %rd98;
	mov.u64 	%rd166, %rd167;
	mov.u32 	%r182, %r183;

BB2_17:
	mov.u32 	%r180, %r182;
	mov.u32 	%r181, %r183;
	mov.u64 	%rd164, %rd166;
	mov.u64 	%rd165, %rd167;
	mov.u64 	%rd144, %rd146;
	setp.lt.u32	%p9, %r28, 4;
	@%p9 bra 	BB2_31;

BB2_18:
	ld.global.u32 	%rd44, [%rd136+1024];
	and.b64  	%rd99, %rd144, -4294967296;
	setp.eq.s64	%p10, %rd99, 0;
	@%p10 bra 	BB2_20;
	bra.uni 	BB2_19;

BB2_20:
	cvt.u32.u64	%r84, %rd44;
	cvt.u32.u64	%r85, %rd144;
	div.u32 	%r86, %r85, %r84;
	rem.u32 	%r87, %r85, %r84;
	cvt.u64.u32	%rd152, %r86;
	cvt.u64.u32	%rd153, %r87;
	bra.uni 	BB2_21;

BB2_19:
	div.u64 	%rd152, %rd144, %rd44;
	rem.u64 	%rd153, %rd144, %rd44;

BB2_21:
	shl.b64 	%rd100, %rd153, 2;
	add.s64 	%rd101, %rd136, %rd100;
	ld.global.u32 	%r88, [%rd101];
	shl.b32 	%r89, %r88, %r181;
	ld.local.u32 	%r90, [%rd165];
	or.b32  	%r91, %r90, %r89;
	st.local.u32 	[%rd165], %r91;
	shl.b32 	%r92, %r174, 8;
	add.s32 	%r93, %r88, %r92;
	cvt.u64.u32	%rd51, %r93;
	mul.wide.u32 	%rd102, %r93, 1028;
	add.s64 	%rd103, %rd80, %rd102;
	ld.global.u32 	%rd52, [%rd103+1024];
	and.b64  	%rd104, %rd152, -4294967296;
	setp.eq.s64	%p11, %rd104, 0;
	@%p11 bra 	BB2_23;
	bra.uni 	BB2_22;

BB2_23:
	cvt.u32.u64	%r94, %rd52;
	cvt.u32.u64	%r95, %rd152;
	div.u32 	%r96, %r95, %r94;
	rem.u32 	%r97, %r95, %r94;
	cvt.u64.u32	%rd154, %r96;
	cvt.u64.u32	%rd155, %r97;
	bra.uni 	BB2_24;

BB2_22:
	div.u64 	%rd154, %rd152, %rd52;
	rem.u64 	%rd155, %rd152, %rd52;

BB2_24:
	add.s32 	%r98, %r174, 1;
	not.b32 	%r99, %r98;
	and.b32  	%r100, %r99, 3;
	shl.b32 	%r101, %r100, 3;
	and.b32  	%r102, %r98, -4;
	cvt.u64.u32	%rd105, %r102;
	add.s64 	%rd106, %rd1, %rd105;
	mul.lo.s64 	%rd107, %rd51, 1028;
	add.s64 	%rd108, %rd80, %rd107;
	shl.b64 	%rd109, %rd155, 2;
	add.s64 	%rd110, %rd108, %rd109;
	ld.global.u32 	%r103, [%rd110];
	shl.b32 	%r104, %r103, %r101;
	ld.local.u32 	%r105, [%rd106];
	or.b32  	%r106, %r105, %r104;
	st.local.u32 	[%rd106], %r106;
	shl.b32 	%r107, %r98, 8;
	add.s32 	%r108, %r103, %r107;
	cvt.u64.u32	%rd59, %r108;
	mul.wide.u32 	%rd111, %r108, 1028;
	add.s64 	%rd112, %rd80, %rd111;
	ld.global.u32 	%rd60, [%rd112+1024];
	and.b64  	%rd113, %rd154, -4294967296;
	setp.eq.s64	%p12, %rd113, 0;
	@%p12 bra 	BB2_26;
	bra.uni 	BB2_25;

BB2_26:
	cvt.u32.u64	%r109, %rd60;
	cvt.u32.u64	%r110, %rd154;
	div.u32 	%r111, %r110, %r109;
	rem.u32 	%r112, %r110, %r109;
	cvt.u64.u32	%rd156, %r111;
	cvt.u64.u32	%rd157, %r112;
	bra.uni 	BB2_27;

BB2_25:
	div.u64 	%rd156, %rd154, %rd60;
	rem.u64 	%rd157, %rd154, %rd60;

BB2_27:
	add.s32 	%r113, %r174, 2;
	not.b32 	%r114, %r113;
	and.b32  	%r115, %r114, 3;
	shl.b32 	%r116, %r115, 3;
	and.b32  	%r117, %r113, -4;
	cvt.u64.u32	%rd114, %r117;
	add.s64 	%rd115, %rd1, %rd114;
	mul.lo.s64 	%rd116, %rd59, 1028;
	add.s64 	%rd117, %rd80, %rd116;
	shl.b64 	%rd118, %rd157, 2;
	add.s64 	%rd119, %rd117, %rd118;
	ld.global.u32 	%r118, [%rd119];
	shl.b32 	%r119, %r118, %r116;
	ld.local.u32 	%r120, [%rd115];
	or.b32  	%r121, %r120, %r119;
	st.local.u32 	[%rd115], %r121;
	shl.b32 	%r122, %r113, 8;
	add.s32 	%r123, %r118, %r122;
	cvt.u64.u32	%rd67, %r123;
	mul.wide.u32 	%rd120, %r123, 1028;
	add.s64 	%rd121, %rd80, %rd120;
	ld.global.u32 	%rd68, [%rd121+1024];
	and.b64  	%rd122, %rd156, -4294967296;
	setp.eq.s64	%p13, %rd122, 0;
	@%p13 bra 	BB2_29;
	bra.uni 	BB2_28;

BB2_29:
	cvt.u32.u64	%r124, %rd68;
	cvt.u32.u64	%r125, %rd156;
	div.u32 	%r126, %r125, %r124;
	rem.u32 	%r127, %r125, %r124;
	cvt.u64.u32	%rd145, %r126;
	cvt.u64.u32	%rd158, %r127;
	bra.uni 	BB2_30;

BB2_28:
	div.u64 	%rd145, %rd156, %rd68;
	rem.u64 	%rd158, %rd156, %rd68;

BB2_30:
	mov.u64 	%rd144, %rd145;
	add.s32 	%r128, %r174, 3;
	shl.b32 	%r129, %r128, 3;
	not.b32 	%r130, %r129;
	and.b32  	%r131, %r130, 24;
	and.b32  	%r132, %r128, -4;
	cvt.u64.u32	%rd123, %r132;
	add.s64 	%rd124, %rd1, %rd123;
	mul.lo.s64 	%rd125, %rd67, 1028;
	add.s64 	%rd126, %rd80, %rd125;
	shl.b64 	%rd127, %rd158, 2;
	add.s64 	%rd128, %rd126, %rd127;
	ld.global.u32 	%r133, [%rd128];
	shl.b32 	%r134, %r133, %r131;
	ld.local.u32 	%r135, [%rd124];
	or.b32  	%r136, %r135, %r134;
	st.local.u32 	[%rd124], %r136;
	shl.b32 	%r137, %r128, 8;
	add.s32 	%r138, %r133, %r137;
	mul.wide.u32 	%rd129, %r138, 1028;
	add.s64 	%rd136, %rd80, %rd129;
	add.s32 	%r175, %r175, 4;
	setp.lt.u32	%p14, %r175, %r28;
	add.s32 	%r174, %r174, 4;
	shl.b32 	%r139, %r174, 3;
	not.b32 	%r140, %r139;
	and.b32  	%r181, %r140, 24;
	and.b32  	%r141, %r174, -4;
	cvt.u64.u32	%rd130, %r141;
	add.s64 	%rd165, %rd1, %rd130;
	mov.u64 	%rd164, %rd165;
	mov.u32 	%r180, %r181;
	@%p14 bra 	BB2_18;

BB2_31:
	mov.u32 	%r142, 255;
	shl.b32 	%r143, %r142, %r180;
	and.b32  	%r144, %r143, %r29;
	ld.local.u32 	%r145, [%rd164];
	or.b32  	%r146, %r145, %r144;
	st.local.u32 	[%rd164], %r146;
	setp.eq.s32	%p15, %r30, 0;
	@%p15 bra 	BB2_33;

	shl.b32 	%r147, %r28, 3;
	st.local.u32 	[%rd1+56], %r147;

BB2_33:
	setp.eq.s32	%p16, %r31, 0;
	@%p16 bra 	BB2_35;

	shl.b32 	%r148, %r28, 3;
	st.local.u32 	[%rd1+60], %r148;

BB2_35:
	mul.lo.s64 	%rd131, %rd2, 36;
	add.s64 	%rd132, %rd78, %rd131;
	ld.local.v4.u32 	{%r149, %r150, %r151, %r152}, [%rd1];
	ld.local.v4.u32 	{%r154, %r155, %r156, %r157}, [%rd1+16];
	st.global.u32 	[%rd132], %r149;
	st.global.u32 	[%rd132+4], %r150;
	st.global.u32 	[%rd132+8], %r151;
	st.global.u32 	[%rd132+12], %r152;
	st.global.u32 	[%rd132+16], %r154;
	st.global.u32 	[%rd132+20], %r155;
	st.global.u32 	[%rd132+24], %r156;
	st.global.u32 	[%rd132+28], %r157;
	st.global.u32 	[%rd132+32], %r28;

BB2_36:
	ret;
}


  